module ConjugateGradient

using LinearAlgebra: dot, norm

struct IterationStep
    n::UInt64
    alpha::Float64
    beta::Float64
    x::Vector{Float64}
    r::Vector{Float64}
    p::Vector{Float64}
end

mutable struct ConvergenceHistory
    isconverged::Bool
    history::Vector{IterationStep}
end

function solve(A, ğ›, ğ±â‚€, Îµ=eps(), maxiter=2000)
    history = ConvergenceHistory(false, [])
    ğ± = ğ±â‚€
    ğ« = ğ› - A * ğ±  # Residual
    ğ© = ğ«  # Momentum
    Î± = compute_alpha(A, ğ«, ğ©)
    Î² = compute_beta(A, ğ«, ğ©)
    push!(history.history, IterationStep(0, Î±, Î², ğ±â‚€, ğ«, ğ©))
    for n in 1:maxiter
        ğ± = ğ± + Î± * ğ©  # Do not do in-place change!
        ğ«â€² = ğ« - Î± * A * ğ©  # Trial move
        if norm(ğ«â€²) < Îµ
            history.isconverged = true
        else
            Î² = compute_beta(ğ«â€², ğ«)
            ğ© = ğ«â€² + Î² * ğ©
            ğ« = ğ«â€²  # Accept the trial move
            push!(history.history, IterationStep(n, Î±, Î², ğ±, ğ«, ğ©))
        end
    end
    return ğ±, history
end

compute_alpha(A, ğ«, ğ©) = dot(ğ«, ğ«) / dot(ğ©, A, ğ©)

compute_beta(A, ğ«, ğ©) = -dot(ğ©, A, ğ«) / dot(ğ©, A, ğ©)
compute_beta(ğ«â‚™â‚Šâ‚, ğ«â‚™) = dot(ğ«â‚™â‚Šâ‚, ğ«â‚™â‚Šâ‚) / dot(ğ«â‚™, ğ«â‚™)

isconverged(ch::ConvergenceHistory) = ch.isconverged

end
